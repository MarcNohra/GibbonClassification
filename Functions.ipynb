{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import vggish_main as vgg\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import itertools\n",
    "# from python_speech_features import mfcc, logfbank\n",
    "# Data augmentation\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Flatten, LSTM\n",
    "from keras.layers import Dropout, Dense, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import imblearn.under_sampling\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Wave Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wav files:\n",
    "def read_wav(file_path):\n",
    "    fs, data = wavfile.read(file_path)\n",
    "    return data, fs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low pass filter using remez\n",
    "def low_pass_remez(fs, cutoff, numtaps=400):\n",
    "    trans_width = 100\n",
    "    taps = signal.remez(numtaps, [0, cutoff, cutoff + trans_width, 0.5*fs], [1, 0], Hz=fs)\n",
    "    return taps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signals(signals):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=True, figsize=(20, 5))\n",
    "    fig.suptitle('Time Series', size=16)\n",
    "    i=0\n",
    "    for y in range(2):\n",
    "        axes[0, y].set_title(list(signals.keys())[i])\n",
    "        axes[0, y].plot(list(signals.keys())[i])\n",
    "        axes[0, y].get_xaxis().set_visible(False)\n",
    "        axes[0, y].get_yaxis().set_visible(False)\n",
    "        i+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(fft):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=True, figsize=(20, 5))\n",
    "    fig.suptitle('Fourier Transforms', size=16)\n",
    "    i=0\n",
    "    for y in range(2):\n",
    "        data=list(fft.values())[i]\n",
    "        Y, freq=data[0], data[1]\n",
    "        axes[0, y].set_title(list(fft.keys())[i])\n",
    "        axes[0, y].plot(freq, Y)\n",
    "        axes[0, y].get_xaxis().set_visible(False)\n",
    "        axes[0, y].get_yaxis().set_visible(False)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fbank(fbank):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=True, figsize=(20, 5))\n",
    "    fig.suptitle('Filter Bank Coefficients', size=16)\n",
    "    i=0\n",
    "    for x in range(1):\n",
    "        for y in range(2):\n",
    "            axes[x, y].set_title(list(fbank.keys())[i])\n",
    "            axes[x, y].imshow(list(fbank.values())[i], cmap='hot', interpolation='nearest')\n",
    "            axes[x, y].get_xaxis().set_visible(False)\n",
    "            axes[x, y].get_yaxis().set_visible(False)\n",
    "            i+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mfccs(mfccs):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=True, figsize=(20, 5))\n",
    "    fig.suptitle('MFCC', size=16)\n",
    "    i=0\n",
    "    for x in range(1):\n",
    "        for y in range(2):\n",
    "            axes[x, y].set_title(list(mfccs.keys())[i])\n",
    "            axes[x, y].imshow(list(mfccs.values())[i], cmap='hot', interpolation='nearest')\n",
    "            axes[x, y].get_xaxis().set_visible(False)\n",
    "            axes[x, y].get_yaxis().set_visible(False)\n",
    "            i+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio features (fft, mfcc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fft(y, rate):\n",
    "    n=len(y)\n",
    "    freq=np.fft.rfftfreq(n, d=1/rate)\n",
    "    Y=abs(np.fft.rfft(y)/n)\n",
    "    return (Y, freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def envelope(y, rate, threshold):\n",
    "    mask=[]\n",
    "    y= pd.Series(y).apply(np.abs)\n",
    "    y_mean=y.rolling(window=int(rate/10), min_periods=1, center=True).mean()\n",
    "    \n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AddGaussianNoise, TimeStretch, PitchShift, Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gaussian Noise\n",
    "def add_gaussian_noise(data_path, file_info, n_repeats=3, min_amp=0.001, max_amp=0.015):\n",
    "    # Create the augmenter\n",
    "    augmenter = Compose([AddGaussianNoise(min_amplitude=min_amp, max_amplitude=max_amp, p=1.0)])\n",
    "    \n",
    "    # Iterate through the Gibbon audio files only\n",
    "    for j in file_info[file_info.label==1].index:\n",
    "        for i in range(n_repeats):\n",
    "            # Read audio file\n",
    "            rate, samples = wavfile.read(data_path + 'Clean/' + file_info.at[j, 'fname'])\n",
    "            # Set the output path\n",
    "            output_file_path = data_path + 'Augmented/AddGaussianNoise_{:03d}_'.format(i) + file_info.at[j, 'fname']\n",
    "            # Add gaussian noise\n",
    "            augmented_samples = augmenter(samples=samples, sample_rate=rate)\n",
    "            # Save the new audio\n",
    "            wavfile.write(filename=output_file_path, rate = rate, data = augmented_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Stretch\n",
    "def time_stretch(data_path, file_info, n_repeats=3, min_rate=0.8, max_rate=1.25):\n",
    "    # Create the augmenter\n",
    "    augmenter = Compose([TimeStretch(min_rate = min_rate, max_rate=max_rate, p=1.0)])\n",
    "    \n",
    "    # Iterate through the Gibbon audio files only\n",
    "    for j in file_info[file_info.label==1].index:\n",
    "        for i in range(n_repeats):\n",
    "            # Read audio file\n",
    "            rate, samples = wavfile.read(data_path + 'Clean/' + file_info.at[j, 'fname'])\n",
    "            # Set the output path\n",
    "            output_file_path = data_path + 'Augmented/TimeStretch_{:03d}_'.format(i) + file_info.at[j, 'fname']\n",
    "            # Perform time stretch\n",
    "            augmented_samples = augmenter(samples=samples, sample_rate=rate)\n",
    "            # Save the new audio\n",
    "            wavfile.write(filename=output_file_path, rate = rate, data = augmented_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch Shift\n",
    "def pitch_shift(data_path, file_info, n_repeats=3, min_semitones=-4, max_semitones=4):\n",
    "    # Create the augmenter\n",
    "    augmenter = Compose([PitchShift(min_semitones=min_semitones, max_semitones=max_semitones, p=1.0)])\n",
    "    \n",
    "    # Iterate through the Gibbon audio files only\n",
    "    for j in file_info[file_info.label==1].index:\n",
    "        for i in range(n_repeats):\n",
    "            # Read audio file\n",
    "            rate, samples = wavfile.read(data_path + 'Clean/' + file_info.at[j, 'fname'])\n",
    "            # Set the output path\n",
    "            output_file_path = data_path + 'Augmented/PitchShift_{:03d}_'.format(i) + file_info.at[j, 'fname']\n",
    "            # Perform time stretch\n",
    "            augmented_samples = augmenter(samples=samples, sample_rate=rate)\n",
    "            # Save the new audio\n",
    "            wavfile.write(filename=output_file_path, rate = rate, data = augmented_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift\n",
    "def shift(data_path, file_info, n_repeats=3, min_fraction=-0.5, max_fraction=0.5):\n",
    "    # Create the augmenter\n",
    "    augmenter = Compose([Shift(min_fraction = min_fraction, max_fraction = max_fraction, p=1.0)])\n",
    "    \n",
    "    # Iterate through the Gibbon audio files only\n",
    "    for j in file_info[file_info.label==1].index:\n",
    "        for i in range(n_repeats):\n",
    "            # Read audio file\n",
    "            rate, samples = wavfile.read(data_path + 'Clean/' + file_info.at[j, 'fname'])\n",
    "            # Set the output path\n",
    "            output_file_path = data_path + 'Augmented/Shift_{:03d}_'.format(i) + file_info.at[j, 'fname']\n",
    "            # Perform time stretch\n",
    "            augmented_samples = augmenter(samples=samples, sample_rate=rate)\n",
    "            # Save the new audio\n",
    "            wavfile.write(filename=output_file_path, rate = rate, data = augmented_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_model(input_shape=128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='random_normal', input_dim=input_shape))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "    model.compile(optimizer ='adam', loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model(input_shape):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same'))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer ='adam', loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrent_model(input_shape):\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(16, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(8, activation='relu')))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer ='adam', loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_recurrent_model(input_shape):\n",
    "    model=Sequential()\n",
    "    # CNN model\n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3), activation='relu', strides=(1, 1),\n",
    "                                     padding='same', input_shape=input_shape)))\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same')))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), padding='same')))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same')))\n",
    "    model.add(TimeDistributed(MaxPool2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "    \n",
    "    # LSTM model\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(16, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(8, activation='relu')))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer ='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method for CNN-LSTM\n",
    "# def method2(input_shape):\n",
    "#     cnn = Sequential()\n",
    "#     cnn.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1),\n",
    "#                                      padding='same', input_shape=input_shape))\n",
    "#     cnn.add(MaxPool2D((2, 2)))\n",
    "#     cnn.add(Flatten())\n",
    "#     # define LSTM model\n",
    "#     model = Sequential()\n",
    "#     model.add(TimeDistributed(cnn, input_shape=(None, 2, 224, 224, 13)))\n",
    "#     model.add(LSTM(128, return_sequences=True))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(optimizer ='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "# #     You aso need to specify a batch size in the input dimensions to that layer I guess,\n",
    "# #     to get the fifth dimension. Try using:\n",
    "# #     model.add(TimeDistributed(cnn, input_shape=(None, num_timesteps, 224, 224,num_chan))).\n",
    "# #     The None will then allow variable batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predictions(directory):\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    \n",
    "    classifier = load_model('vggish_binary_class_model.h5')\n",
    "    y_pred_unseen = classifier.predict(postprocessed_batch_appended)\n",
    "    y_pred_unseen = (y_pred_unseen>0.5)\n",
    "    # Accuracy of the unseen data\n",
    "    cm_unseen = confusion_matrix(y[:, 0], y_pred_unseen)\n",
    "    print(cm_unseen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract VGG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vgg_features(file_info, data_path):\n",
    "    # Iterate through the files\n",
    "    for i in tqdm(file_info.index):\n",
    "        # If the file has already been converted skip it\n",
    "        if(file_info.at[i, 'vgg_done']==1):\n",
    "            continue\n",
    "        vgg_graph = tf.Graph()\n",
    "        with vgg_graph.as_default():\n",
    "            session_vgg = tf.Session()\n",
    "            with session_vgg.as_default():\n",
    "                vgg_net = vgg.CreateVGGishNetwork(session_vgg)\n",
    "                vgg_mfcc, _ = vgg.ProcessWithVGGish(session_vgg, vgg_net,\n",
    "                                                    data_path+'Clean/'+file_info.at[i, 'fname'])\n",
    "                # Save the vgg features locally on machine\n",
    "                with open(data_path+'VGG/' + file_info.at[i, 'fname'], 'wb') as f:\n",
    "                    pickle.dump(vgg_mfcc, f)\n",
    "                file_info.at[i, 'vgg_done']=1\n",
    "        # Save the updated file info and return it\n",
    "        file_info.loc[:, file_info.columns!='length'].to_csv(data_path+'files_labels.csv', index=False)\n",
    "    return file_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Undersampling\n",
    "def random_undersampling(data_X, data_y, ratio):\n",
    "    # The ratio represents the amount of data in the minority class compared to the majority one\n",
    "    rand_under_samp = imblearn.under_sampling.RandomUnderSampler(sampling_strategy=ratio)\n",
    "    under_samp_X, under_samp_y = rand_under_samp.fit_resample(data_X, data_y)\n",
    "    \n",
    "    return under_samp_X, under_samp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE and EN resmapling\n",
    "def smote_en_resamp(data_X, data_y, k_neighbors=5):\n",
    "    # Perform under and over sampling using SMOTE and EN\n",
    "    smote = SMOTE(sampling_strategy='minority', k_neighbors=k_neighbors, n_jobs=8)\n",
    "    enn = EditedNearestNeighbours(n_neighbors=k_neighbors, n_jobs=8)\n",
    "    smoteen = SMOTEENN(sampling_strategy=\"minority\", smote=smote, enn=enn, n_jobs=8)\n",
    "    resamp_X, resamp_y = smoteen.fit_sample(data_X, data_y)\n",
    "    \n",
    "    return resamp_X, resamp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select files for test and train data\n",
    "# This function returns the file names only\n",
    "def build_rand_feat(file_info, training_ratio=0.75):\n",
    "    training_files=[]\n",
    "    unseen_files=[]\n",
    "    \n",
    "    rand_files=file_info.groupby(['label'])['fname'].apply(lambda x: np.random.permutation(x))\n",
    "\n",
    "    for j in range(rand_files.shape[0]):\n",
    "        # Get the number of files in the training and testing variables\n",
    "        files_cnt=len(rand_files[j])\n",
    "        train_samp=math.ceil(files_cnt*training_ratio)\n",
    "\n",
    "        training_files.append(rand_files[j][0:train_samp])\n",
    "        unseen_files.append(rand_files[j][train_samp:files_cnt])\n",
    "    \n",
    "    # Convert them to a 1d list\n",
    "    training_files=list(itertools.chain.from_iterable(training_files))\n",
    "    unseen_files=list(itertools.chain.from_iterable(unseen_files))\n",
    "    \n",
    "    # Shuffle the data\n",
    "    training_files=np.random.permutation(training_files)\n",
    "    unseen_files=np.random.permutation(unseen_files)\n",
    "    \n",
    "    return training_files, unseen_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation\n",
    "# model_type can be: lstm, binary\n",
    "def cross_validation(model_type, file_info, training_files, data_path, model_name_prefix,\n",
    "                     nfolds=5, lstm_steps=[], thresholds=[0.5], class_weight=True,\n",
    "                     rand_under_samp=True, under_samp_ratio=0.3, smote_en_resamp=True, smote_en_neighbors=5):\n",
    "    # Get the number of testing files based on the number of folds\n",
    "    num_files = len(training_files)\n",
    "    num_test_files = math.ceil(num_files/nfolds)\n",
    "    test_start_index = 0\n",
    "    model = None\n",
    "    \n",
    "    conf_matrix = []\n",
    "    recall = []\n",
    "    \n",
    "    for i in range(nfolds):\n",
    "        print('####### Fold: ', i)\n",
    "        train_X = train_y = test_X = test_y = []\n",
    "        # set the starting index\n",
    "        test_start_index = i*num_test_files\n",
    "        # Set the testing and training indexes\n",
    "        test_ind = range(test_start_index, (test_start_index+num_test_files))\n",
    "        # Get the indexes that are not in the testing range\n",
    "        train_ind = [ind for ind in range(num_files) if ind not in test_ind]\n",
    "        \n",
    "        print('####### Reading testing data')\n",
    "        # Read the testing data\n",
    "        for j in test_ind:\n",
    "            # Get the file name and read the data\n",
    "            file = file_info[file_info.fname == training_files[j]].fname.iloc[0]\n",
    "            test_X.append(pickle.load(open(data_path+'/VGG/'+ file, 'rb')))\n",
    "            # Get the class label\n",
    "            label = file_info[file_info.fname == training_files[j]].label.iloc[0]\n",
    "            test_y.append([label]*(test_X.shape[0]))\n",
    "        \n",
    "        print('####### Reading training data')\n",
    "        # Read the training data\n",
    "        for j in train_ind:\n",
    "            # Get the file name and read the data\n",
    "            file = file_info[file_info.fname == training_files[j]].fname.iloc[0]\n",
    "            train_X.append(pickle.load(open(data_path+'/VGG/'+ file, 'rb')))\n",
    "            # Get the class label\n",
    "            label = file_info[file_info.fname == training_files[j]].label.iloc[0]\n",
    "            train_y.append([label]*(train_X.shape[0]))\n",
    "        \n",
    "        # Perform resampling techniques inside the cross validation to maximise randomness\n",
    "        # and reduce over-fitting\n",
    "        if rand_under_samp == True:\n",
    "            print('####### Random Undersamp')\n",
    "            train_X, train_y = random_undersampling(train_X, train_y, under_samp_ratio)\n",
    "        \n",
    "        if smote_en_resamp == True:\n",
    "            print('####### Smote_En')\n",
    "            train_X, train_y = smote_en_resamp(train_X, train_y, smote_en_neighbors)\n",
    "        \n",
    "        # Save the data\n",
    "        with open(data_path + 'CV_Data/' + model_name_prefix + '_train_CV' + str(i), 'wb') as f:\n",
    "            pickle.dump([train_X, train_y], f)\n",
    "        with open(data_path + 'CV_Data/' + model_name_prefix + '_test_CV' + str(i), 'wb') as f:\n",
    "            pickle.dump([test_X, test_y], f)\n",
    "        print('####### Data saved')\n",
    "        \n",
    "        # Fit the model\n",
    "        print('####### Model fitting')\n",
    "        if(model_type == 'lstm'):\n",
    "            for step in lstm_steps:\n",
    "                print('####### Step: ', step)\n",
    "                # Reshape the input data\n",
    "                input_shape = (step, 128)\n",
    "                train_x = train_x.reshape(-1, step, 128)\n",
    "                test_X = test_X.reshape(-1, step, 128)\n",
    "                # Get the model and train it\n",
    "                model = get_recurrent_model(input_shape)\n",
    "                if class_weight == True:\n",
    "                    classes = list(np.unique(file_info.label))\n",
    "                    class_weight = compute_class_weight('balanced', classes, train_y)\n",
    "                    model.fit(train_x, train_y, epochs = 50, batch_size = 32,\n",
    "                              class_weight=class_weight, verbose=1)\n",
    "                else:\n",
    "                    model.fit(train_x, train_y, epochs = 50, batch_size = 32, verbose=1)\n",
    "                # Save the model\n",
    "                class_file_name = model_name_prefix + '_CV' + str(i) + '_step' + str(step) + '.h5'\n",
    "                model.save(data_path + 'Models/' + class_file_name)\n",
    "                print('####### Model saved')\n",
    "                \n",
    "                print('####### Model testing')\n",
    "                # Testing the model\n",
    "                y_pred = model.predict(test_X)\n",
    "                for threshold in thresholds:\n",
    "                    print('####### Threshold: ', threshold)\n",
    "                    y_pred_th = (y_pred > threshold)\n",
    "                    \n",
    "                    # Confusion matrix\n",
    "                    cm_test = confusion_matrix(test_y, y_pred_th)\n",
    "                    print(cm_test)\n",
    "                    conf_matrix = np.append(conf_matrix, cm_test)\n",
    "                    # Recall\n",
    "                    tpr_test = recall_score(test_y, y_pred_th)\n",
    "                    print('TPR: ', tpr_test)\n",
    "                    recall = np.append(recall, tpr_test)\n",
    "            \n",
    "    return conf_matrix, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
